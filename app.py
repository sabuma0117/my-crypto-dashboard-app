import streamlit as st
import requests
import feedparser # feedparserã‚’è¿½åŠ 
import json
import time
from datetime import datetime

# --- è¨­å®šé …ç›® ---
# ä¾¡æ ¼å–å¾—ã‚³ã‚¤ãƒ³ID (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
default_coin_ids = ['ethereum', 'solana', 'sui', 'ondo-finance']
# ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
default_keywords = [
    'eth', 'ethereum', 'sol', 'solana', 'sui', 'ondo', 'bitcoin', 'btc',
    'è¦åˆ¶', 'ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ', 'nft', 'defi', 'web3', 'ãƒã‚¤ãƒŠãƒ³ã‚¹',
]
# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å…ƒRSSãƒ•ã‚£ãƒ¼ãƒ‰
rss_feeds = {
    "CoinTelegraph Japan": "https://jp.cointelegraph.com/rss",
    "ã‚ãŸã‚‰ã—ã„çµŒæ¸ˆ": "https://www.neweconomy.jp/feed",
}
# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—/è¡¨ç¤ºè¨­å®š
max_entries_to_fetch = 20
max_entries_to_display = 10
# User-Agentãƒ˜ãƒƒãƒ€ãƒ¼
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
# --- è¨­å®šé …ç›®ã“ã“ã¾ã§ ---

# --- é–¢æ•°å®šç¾© ---
# (get_price_dataé–¢æ•°ã¯å‰å›ã¨åŒã˜ãªã®ã§çœç•¥)
@st.cache_data(ttl=600) # ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœã‚’10åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
def get_price_data(coin_ids):
    """CoinGecko APIã‹ã‚‰ä¾¡æ ¼æƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•° (Streamlitç‰ˆ)"""
    price_data_list = []
    if not coin_ids: return [] # å¯¾è±¡ãŒãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆ

    st.write("ä¾¡æ ¼æƒ…å ± å–å¾—ä¸­...")
    url = f"https://api.coingecko.com/api/v3/coins/markets?vs_currency=jpy&ids={','.join(coin_ids)}"
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data:
            for coin_data in data:
                 price_info = {
                     'id': coin_data.get('id', 'N/A'),
                     'name': coin_data.get('name', 'N/A'),
                     'current_price': coin_data.get('current_price', 0),
                     'price_change_percentage_24h': coin_data.get('price_change_percentage_24h', 0),
                     'market_cap': coin_data.get('market_cap', 0)
                 }
                 price_data_list.append(price_info)
            st.success("ä¾¡æ ¼æƒ…å ± å–å¾—å®Œäº†ï¼")
            return price_data_list
        else:
            st.error(f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸã‚³ã‚¤ãƒ³ID ({','.join(coin_ids)}) ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return []
    except requests.exceptions.RequestException as e:
        st.error(f"ä¾¡æ ¼APIã‚¨ãƒ©ãƒ¼: {e}")
        return []
    except Exception as e:
        st.error(f"ä¾¡æ ¼APIäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
        return []


# --- â˜…â˜…â˜… ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•° (Streamlitç‰ˆ) ã‚’è¿½åŠ  â˜…â˜…â˜… ---
@st.cache_data(ttl=600) # ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœã‚’10åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
def get_filtered_news(keywords):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹é–¢æ•° (Streamlitç‰ˆ)"""
    filtered_news_list = []
    if not keywords: return [] # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆ

    st.write("\nãƒ‹ãƒ¥ãƒ¼ã‚¹ å–å¾—ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")
    total_fetched_count = 0

    for site_name, feed_url in rss_feeds.items():
        # st.write(f"[{site_name}] å‡¦ç†ä¸­...") # è©³ç´°ãƒ­ã‚°ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã‚‚OK
        feed_content = None
        try:
            response = requests.get(feed_url, headers=headers, timeout=15)
            response.raise_for_status()
            feed_content = response.text
        except requests.exceptions.RequestException as e:
            st.warning(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼ ({site_name}): {e}") # ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãè­¦å‘Šã«å¤‰æ›´
            continue
        except Exception as e:
             st.warning(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ ({site_name}): {e}")
             continue

        if feed_content:
            try:
                parsed_data = feedparser.parse(feed_content)
                if parsed_data.bozo:
                     st.warning(f"ãƒ•ã‚£ãƒ¼ãƒ‰è§£æã«å•é¡Œã‚ã‚Š ({site_name}) - {parsed_data.bozo_exception}")

                if not parsed_data.entries:
                     # st.write(f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãªã— ({site_name})") # ãƒ­ã‚°ã¯çœç•¥å¯
                     continue

                for entry in parsed_data.entries[:max_entries_to_fetch]:
                    title = entry.get('title', '')
                    link = entry.get('link', '#')
                    title_lower = title.lower()
                    matched_keyword = ""
                    for keyword in keywords: # å¼•æ•°ã§å—ã‘å–ã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                        if keyword in title_lower:
                            matched_keyword = keyword
                            break

                    if matched_keyword:
                        published_time_str = "æ—¥æ™‚ä¸æ˜"
                        published_parsed = entry.get('published_parsed') or entry.get('updated_parsed')
                        if published_parsed:
                            try:
                                published_time_str = time.strftime('%Y-%m-%d %H:%M', published_parsed)
                            except ValueError:
                                published_time_str = "ä¸æ­£ãªæ—¥æ™‚"

                        filtered_news_list.append({
                            'site': site_name,
                            'keyword': matched_keyword,
                            'title': title,
                            'link': link,
                            'published': published_time_str
                        })
                        total_fetched_count += 1
            except Exception as e:
                st.warning(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹è§£æã‚¨ãƒ©ãƒ¼ ({site_name}): {e}")

    # ã‚½ãƒ¼ãƒˆå‡¦ç† (å‰å›åŒæ§˜)
    def get_sort_key(news_item):
        try:
            return datetime.strptime(news_item.get('published', ''), '%Y-%m-%d %H:%M')
        except (ValueError, TypeError):
             return datetime.min
    filtered_news_list.sort(key=get_sort_key, reverse=True)

    st.success(f"é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(filtered_news_list)}ä»¶ åé›†å®Œäº†ï¼")
    return filtered_news_list
# --- â˜…â˜…â˜… é–¢æ•°è¿½åŠ ã“ã“ã¾ã§ â˜…â˜…â˜… ---


# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ† ---

st.set_page_config(page_title="ä»®æƒ³é€šè²¨æƒ…å ±ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", layout="wide")
st.title('ğŸš€ ä»®æƒ³é€šè²¨æƒ…å ±ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰')

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (è¨­å®šç”¨) ---
st.sidebar.header('âš™ï¸ è¨­å®š')
# ã‚³ã‚¤ãƒ³IDå…¥åŠ› (è¤‡æ•°é¸æŠå¯èƒ½ã«ã™ã‚‹å ´åˆã¯ st.multiselect ãŒä¾¿åˆ©ã ãŒã€ã¾ãšã¯ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›)
coin_ids_input = st.sidebar.text_input('ä¾¡æ ¼å–å¾—ã‚³ã‚¤ãƒ³ID (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)', value=','.join(default_coin_ids))
coin_ids_to_fetch = [coin_id.strip() for coin_id in coin_ids_input.split(',') if coin_id.strip()]
if not coin_ids_to_fetch:
    st.sidebar.warning("ã‚³ã‚¤ãƒ³IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    coin_ids_to_fetch = default_coin_ids

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
keywords_input = st.sidebar.text_input('ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)', value=','.join(default_keywords))
keywords_to_use = [keyword.strip().lower() for keyword in keywords_input.split(',') if keyword.strip()]
if not keywords_to_use:
    st.sidebar.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    keywords_to_use = default_keywords

# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---

# --- ä¾¡æ ¼è¡¨ç¤º ---
st.header('ğŸ“ˆ ä¾¡æ ¼æƒ…å ±')
# æ›´æ–°ãƒœã‚¿ãƒ³
if st.button('ğŸ”„ ãƒ‡ãƒ¼ã‚¿æ›´æ–°'):
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†å®Ÿè¡Œ (st.cache_data ã‚’ä½¿ã£ã¦ã„ã‚‹å ´åˆ)
    st.cache_data.clear()
    st.experimental_rerun()

price_data = get_price_data(coin_ids_to_fetch) # å…¥åŠ›ã•ã‚ŒãŸã‚³ã‚¤ãƒ³IDã‚’æ¸¡ã™

if price_data:
    cols = st.columns(4)
    col_index = 0
    for price_info in price_data:
        current_col = cols[col_index % len(cols)]
        current_col.metric(
            label=f"{price_info.get('name', 'N/A')} ({price_info.get('id', 'N/A').capitalize()})",
            value=f"{price_info.get('current_price', 0):,.2f} JPY",
            delta=f"{price_info.get('price_change_percentage_24h', 0):.2f}%"
        )
        col_index += 1
    # è©³ç´°ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
    with st.expander("è©³ç´°ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
        st.dataframe(price_data, use_container_width=True)
else:
    st.warning('ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚')


# --- â˜…â˜…â˜… ãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¤ºã‚’è¿½åŠ  â˜…â˜…â˜… ---
st.header('ğŸ“° é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹')
st.write(f"(ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords_to_use)})")

# ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— (å…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¸¡ã™)
news_data = get_filtered_news(keywords_to_use)

if news_data:
    # è¡¨ç¤ºä¸Šé™ã¾ã§ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¡¨ç¤º
    for i, news_item in enumerate(news_data[:max_entries_to_display]):
        site = news_item.get('site', 'ä¸æ˜ã‚µã‚¤ãƒˆ')
        keyword = news_item.get('keyword', '')
        title = news_item.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')
        link = news_item.get('link', '#')
        published = news_item.get('published', 'æ—¥æ™‚ä¸æ˜')

        # st.expanderã‚’ä½¿ã†ã¨ã€ã‚¯ãƒªãƒƒã‚¯ã§è©³ç´°ï¼ˆãƒªãƒ³ã‚¯ï¼‰ã‚’è¡¨ç¤ºã§ãã‚‹
        with st.expander(f"[{site} / {keyword}] ({published}) {title}"):
            st.markdown(f"[{link}]({link})") # markdownå½¢å¼ã§ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º

    if len(news_data) > max_entries_to_display:
         st.caption(f"ï¼ˆå…¨{len(news_data)}ä»¶ä¸­ã€æœ€æ–°{max_entries_to_display}ä»¶ã‚’è¡¨ç¤ºï¼‰")

else:
    st.info('é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')
# --- â˜…â˜…â˜… ãƒ‹ãƒ¥ãƒ¼ã‚¹è¡¨ç¤ºã“ã“ã¾ã§ â˜…â˜…â˜… ---


# --- ãƒ•ãƒƒã‚¿ãƒ¼çš„ãªæƒ…å ± ---
st.sidebar.write("---")
st.sidebar.write(f"æœ€çµ‚æ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")